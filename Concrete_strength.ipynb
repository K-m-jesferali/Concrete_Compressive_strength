{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PRCP-1019-ConcreteStren"
      ],
      "metadata": {
        "id": "3RjVN8IiDiDk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "- Prepare a complete data analysis report on the concrete data.\n",
        "- Create a machine learning model which can predict the future strength of a concrete mix, based on its constituents’ composition and also the age of the mix."
      ],
      "metadata": {
        "id": "K4F-9OmcGrak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Overview\n",
        "\n",
        "Concrete as a building block of most construction is the most important material in civil engineering. The concrete compressive strength is a highly nonlinear function of age and ingredients. These ingredients include cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and fine aggregate.\n",
        "The actual concrete compressive strength (MPa) for a given mixture under a specific age (days) was determined from the laboratory. Data is in raw form (not scaled).The data has 8 quantitative input variables, and 1 quantitative output variable, and 1030 instances (observations).\n"
      ],
      "metadata": {
        "id": "X05Lzi8pG0V8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features and its Description\n",
        "\n",
        "\n",
        "| **Feature Name**           | **Unit**     | **Description**                                                            |\n",
        "| -------------------------- | ------------ | -------------------------------------------------------------------------- |\n",
        "| **Cement**   | kg/m³        | Primary binding material; higher amounts generally increase strength.      |\n",
        "| **Blast Furnace Slag** | kg/m³        | Supplementary cement material; improves durability and long-term strength. |\n",
        "| **Fly Ash**  | kg/m³        | Cement substitute; enhances workability and long-term strength.            |\n",
        "| **Water**    | kg/m³        | Required for hydration; too much reduces strength due to higher porosity.  |\n",
        "| **Superplasticizer**   | kg/m³        | Chemical admixture; improves workability while keeping water content low.  |\n",
        "| **Coarse Aggregate**   | kg/m³        | Gravel/crushed stone; contributes to concrete strength and bulk.           |\n",
        "| **Fine Aggregate**     | kg/m³        | Sand-like material; fills gaps, affects workability and surface finish.    |\n",
        "| **Age**                    | Days (1–365) | Time since casting; strength increases over time due to ongoing hydration. |\n",
        "| **Compressive Strength**   | MPa          | Target variable; measures the load-bearing capacity of concrete.           |\n"
      ],
      "metadata": {
        "id": "aaRZ7j7LEu8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "3XLHrwRmDsmM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLT0zFk-O_uu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data reading"
      ],
      "metadata": {
        "id": "vS2kiZ9-Wy7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/concrete.csv')"
      ],
      "metadata": {
        "id": "_Nu8PQLePU9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "4UeaB-4gVhNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "qfKooNyeVjrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "sIvTkaNdDzXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "483pT-fbWgAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "2UboUo8iW_8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling the missing values\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "Y-0cmEhsgxLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Handling Duplicate values\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "WDwf1nwohogD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "cA56-lnvhtXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "A5d8Dveph0Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outliers analysis\n",
        "columns = df.columns.tolist()[:-1]\n",
        "plt.figure(figsize=(15,15),facecolor='white')\n",
        "plotnumber = 1\n",
        "for column in columns:\n",
        "  if plotnumber<=9:\n",
        "    plt.subplot(3,3,plotnumber)\n",
        "    sns.boxplot(df[column])\n",
        "    plt.xlabel(column,fontsize = 20)\n",
        "    plt.ylabel('count',fontsize = 20)\n",
        "    plotnumber+=1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "4y-7jFN7dzVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (25, 20))\n",
        "plotnumber = 1\n",
        "\n",
        "for col in columns:\n",
        "    if plotnumber <= 9:\n",
        "        ax = plt.subplot(3, 3, plotnumber)\n",
        "        sns.distplot(df[col])\n",
        "        plt.xlabel(col, fontsize = 15)\n",
        "\n",
        "    plotnumber += 1\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kHJtbqW6Eh8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see some outliers are present in water, Superplastic and Age columns, I will handle these outliers later in feature engineering part."
      ],
      "metadata": {
        "id": "pgDFTmIJE1RK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "EZrQhgHJXt_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Univariate analysis\n",
        "columns = df.columns.tolist()[:-1]\n",
        "plt.figure(figsize=(15,15),facecolor='white')\n",
        "plotnumber = 1\n",
        "for column in columns:\n",
        "  if plotnumber<=9:\n",
        "    plt.subplot(3,3,plotnumber)\n",
        "    sns.histplot(df[column],kde=True)\n",
        "    plt.xlabel(column,fontsize = 20)\n",
        "    plt.ylabel('count',fontsize = 20)\n",
        "    plotnumber+=1\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "TJHPi6BVXoeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bi variate analysis\n",
        "#- which features contributes more in predicting the target column"
      ],
      "metadata": {
        "id": "UfeCkKokYMG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25,25),facecolor='white')\n",
        "plotnumber = 1\n",
        "for column in columns:\n",
        "  if plotnumber<=9:\n",
        "    plt.subplot(3,3,plotnumber)\n",
        "    sns.scatterplot(df[column])\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('concrete_strength')\n",
        "    plotnumber+=1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "hqKymwUzeUiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi variate analysis"
      ],
      "metadata": {
        "id": "C2B-bkvpbQaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Correlation Analysis\n",
        "#Heat map for correlation\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(df.corr(),annot = True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8PEl7VW8gGZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights from Correlation Matrix :**\n",
        "\n",
        " - Cement and Concrete Compressive Strength: The correlation is 0.49(0.50), which is a moderate positive correlation. This indicates that as the amount of cement increases, the compressive strength of the concrete tends to increase as well.\n",
        "\n",
        "- Water and Superplasticizer: There is a strong negative correlation of -0.65(-0.66). This suggests that the more superplasticizer used, the less water is needed. Superplasticizers are used to enhance the workability of concrete, allowing for a reduction in water content without reducing fluidity.\n",
        "\n",
        "- Fly Ash and Superplasticizer: These have a correlation of 0.41(0.38), a moderate positive correlation, implying that fly ash and superplasticizer quantities tend to increase together. Fly ash can improve workability and reduce water content, which might be why it's used in conjunction with superplasticizers.\n",
        "\n",
        "- Fine Aggregate and Water: This pair has a correlation of -0.44(-0.45), indicating a moderate negative correlation. It implies that an increase in the amount of fine aggregate may be associated with a decrease in water content.\n",
        "\n",
        "- Age and Concrete Compressive Strength: With a correlation of 0.34(0.33), it indicates a positive relationship, albeit not very strong, suggesting that as the concrete ages, its compressive strength tends to increase, which is expected as concrete gains strength over time.\n",
        "\n",
        "- Blast Furnace Slag and Fly Ash: There's a negative correlation of -0.31(-0.32), which could indicate that in mixtures where blast furnace slag is used, less fly ash is present, and vice versa.\n",
        "\n",
        "- Water and Cement: The correlation is -0.057(-0.08), which is a very weak negative correlation, suggesting that there is no significant relationship between the amounts of water and cement used in the concrete mix."
      ],
      "metadata": {
        "id": "_2QKLcO9GkQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pair plot\n",
        "sns.pairplot(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5G07dM0HbyN8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - This pairplot provides a comprehensive visualization of the relationship between each pair of variables in the dataset. Here's an analysis of the insights that I have derived from this plot:\n",
        "\n",
        "- Distribution of Individual Variables:\n",
        "\n",
        " - The histograms along the diagonal show the distribution of single variables\n",
        "Cement, blast furnace slag, and fly ash display a somewhat right-skewed distribution, indicating a concentration of lower values with some higher outliers.\n",
        "\n",
        " - Water, superplasticizer, and age show a near-normal or uniform distribution.\n",
        "Coarse aggregate and fine aggregate are left-skewed, with higher frequencies of larger quantities.\n",
        "\n",
        " - Concrete compressive strength appears normally distributed, which is ideal for many statistical analysis methods that assume normality.\n",
        "\n",
        "- Pairwise Relationships:\n",
        "The scatter plots off the diagonal show the relationships between pairs of variables.\n",
        "\n",
        " - There are some variables that show a pattern suggesting a correlation, like cement and concrete compressive strength, where an increase in cement seems to be associated with higher compressive strength.\n",
        "\n",
        " - Age and compressive strength show a non-linear pattern where strength increases with age up to a certain point before leveling off, which is consistent with the curing process of concrete.\n",
        "\n",
        " - The negative relationship between water and superplasticizer is also visible, supporting the idea that superplasticizers are effective in reducing water content while maintaining workability.\n",
        "\n",
        " - Some variables show no discernible pattern or relationship, like coarse aggregate with many other components, indicating a lack of correlation.\n",
        "\n",
        "- Data Density:\n",
        " - The density of points within scatter plots varies, with some areas being more densely populated. This indicates the commonality of certain mix proportions in the dataset."
      ],
      "metadata": {
        "id": "jtvmmky1IeVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kxZDDicMhSGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking multi-collinearity\n",
        "# Rule of thumb:\n",
        "\n",
        "# VIF > 5 → Moderate multicollinearity\n",
        "\n",
        "# VIF > 10 → Severe multicollinearity (consider removing variable)\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from statsmodels.tools.tools import add_constant\n",
        "X = df.drop('strength',axis=1)\n",
        "X = add_constant(X)\n",
        "\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = X.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "print(vif)\n"
      ],
      "metadata": {
        "id": "hZa3FA-FlbUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spliting the independent and non-indepentent features\n",
        "x = df.drop('strength',axis=1)\n",
        "y = df['strength']"
      ],
      "metadata": {
        "id": "SNRevTchoQFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.var()"
      ],
      "metadata": {
        "id": "uQJJskOgKqrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalizing features\n",
        "# let's add 1 to each value in everycolumn so that we don't get exception while calculating the log value of 0\n",
        "\n",
        "for column in x.columns:\n",
        "    x[column] += 1\n",
        "    x[column] = np.log(x[column])"
      ],
      "metadata": {
        "id": "hFhtQLhNKsoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.var()"
      ],
      "metadata": {
        "id": "O_7fyBPHK1LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for Outliers\n",
        "plt.figure(figsize = (20, 15))\n",
        "plotnumber = 1\n",
        "\n",
        "for col in x.columns:\n",
        "    if plotnumber <= 8:\n",
        "        ax = plt.subplot(3, 3, plotnumber)\n",
        "        sns.boxplot(X[col])\n",
        "        plt.xlabel(col, fontsize = 15)\n",
        "\n",
        "    plotnumber += 1\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eNXCZttVLMFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaling"
      ],
      "metadata": {
        "id": "J0UFmgs5CaCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Spliting the data for training and testing\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "Ddvl6qU9oUgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#StandardScaler scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)"
      ],
      "metadata": {
        "id": "m3ZdRQNctRcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#After standardscaling , Creating dataframe for x_train and x_test.\n",
        "x_train = pd.DataFrame(x_train,columns = x.columns)\n",
        "x_test  = pd.DataFrame(x_test,columns = x.columns)\n",
        "print(x_train.head())\n",
        "print(x_test.head())"
      ],
      "metadata": {
        "id": "Je-K7abIthEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation and Evaluation"
      ],
      "metadata": {
        "id": "f2jngE4YnG-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error, r2_score\n",
        "\n",
        "\n",
        "\n",
        "# Models\n",
        "models = {\n",
        "    \"KNN\": KNeighborsRegressor(),\n",
        "    \"Desicion Tree\": DecisionTreeRegressor(),\n",
        "    \"Random Forest\": RandomForestRegressor(),\n",
        "    \"AdaBoost\": AdaBoostRegressor(),\n",
        "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
        "    \"XGBoost\": XGBRegressor(),\n",
        "}\n",
        "\n",
        "#train and evaluate each model\n",
        "for name, model in models.items():\n",
        "    model.fit(x_train, y_train)\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"{name}:\\n\\tMSE = {mse:.2f},\\n\\tMAE = {mae:.2f}, R² = {r2:.2f}\\n\")\n"
      ],
      "metadata": {
        "id": "Pf5xrLWuodRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the all the model we evaluated, XGBoostRegressor has highest accuracy.\n",
        "\n",
        "Let's do the Hyperparameter tuning for XGBoostRegressor"
      ],
      "metadata": {
        "id": "wdGlyRYKICKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Define model\n",
        "model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1, 0.3],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=model,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=3,\n",
        "                           scoring='neg_mean_squared_error',\n",
        "                           verbose=1,\n",
        "                           n_jobs=-1)\n",
        "\n",
        "grid_search.fit(x_train, y_train)\n",
        "\n",
        "#  Best model and score\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best RMSE on training:\", (-grid_search.best_score_)**0.5)\n",
        "\n",
        "#  Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(x_test)\n",
        "rmse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Test RMSE:\", rmse)\n",
        "\n",
        "# Evaluate r2 score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"R-squared (R² Score):\", r2)\n"
      ],
      "metadata": {
        "id": "S5bmWNX224Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = grid_search.best_params_"
      ],
      "metadata": {
        "id": "iIeulm_RE0KV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBRegressor\n",
        "model = XGBRegressor(**best_params,objective='reg:squarederror', random_state=42)\n",
        "model.fit(x_train,y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "print('Accuracy:',r2_score(y_test,y_pred))"
      ],
      "metadata": {
        "id": "zzeLn7CvTavV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'Actual':y_test,'Predicted':y_pred , 'Error':y_test-y_pred})"
      ],
      "metadata": {
        "id": "kjkID2umEVGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's give a new data to the model to predict\n",
        "cement = float(input('Enter the value of cement: '))\n",
        "slag = float(input('Enter the value of blast_furnace_slag: '))\n",
        "ash = float(input('Enter the value of fly_ash: '))\n",
        "water = float(input('Enter the value of water: '))\n",
        "superplastic = float(input('Enter the value of superplasticizer: '))\n",
        "coarseagg = float(input('Enter the value of coarse_aggregate: '))\n",
        "fineagg = float(input('Enter the value of fine_aggregate: '))\n",
        "age = float(input('Enter the value of the age :'))\n",
        "\n",
        "new_val = np.array([cement,slag,ash,water,superplastic,coarseagg,fineagg,age])\n",
        "new_val = new_val.reshape(1,-1)\n",
        "new_val = mm.transform(new_val)\n",
        "strength = model.predict(new_val)\n",
        "print(f\"The strength of the concrete is :{strength}\")\n"
      ],
      "metadata": {
        "id": "2jYiw2DmI9He"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Comparison report"
      ],
      "metadata": {
        "id": "VqxojUbSBfyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have tested 6 Regression models to predict the concrete strength. Here the detailed report.\n",
        "\n",
        "K-Nearest Neighbors (KNN):\n",
        "\n",
        "- Moderate performance with R² = 0.84 and relatively high MSE (46.68).\n",
        "\n",
        "- This model may struggle to generalize, especially in datasets with higher dimensionality.\n",
        "\n",
        "Decision Tree:\n",
        "\n",
        "- Simple and interpretable model with R² = 0.88. However, it can overfit the training data, which limits its generalization ability compared to ensembles.\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "- Strong performer with R² = 0.91, lower error values than Decision Tree and KNN.\n",
        "\n",
        "- Offers robustness and good generalization due to its ensemble nature.\n",
        "\n",
        "AdaBoost:\n",
        "\n",
        "- The weakest model here, with highest MSE (65.21) and lowest R² (0.78).\n",
        "\n",
        "- Tends to be sensitive to noisy data and outliers in regression tasks.\n",
        "\n",
        "Gradient Boosting:\n",
        "\n",
        "- Performs well with R² = 0.89, but not as accurate as Random Forest or XGBoost.\n",
        "\n",
        "XGBoost:\n",
        "\n",
        "- Clearly the best performing model, with the lowest MSE (20.39), lowest MAE (2.76), and highest R² (0.93).\n",
        "\n",
        "- Optimized for both speed and accuracy, XGBoost handles feature importance and regularization effectively, minimizing overfitting while maintaining strong performance.\n",
        "\n",
        "Hyperparameter Tuning:\n",
        "\n",
        "- To further enhance model performance, we applied hyperparameter tuning using techniques such as Grid Search on the XGBoost model. After optimization, the model achieved an improved R² score of 0.944, indicating even better predictive capability.\n",
        "\n",
        "Result:\n",
        "\n",
        "- Deploy XGBoost as the primary model for production.\n",
        "\n",
        "- It demonstrates the best predictive accuracy and generalization capability, making it the most reliable choice for predicting concrete compressive strength in real-world applications.\n"
      ],
      "metadata": {
        "id": "yS3ZAJBsBvIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Report on Challenges faced"
      ],
      "metadata": {
        "id": "geVn8OFzEatB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\tThe main challenge was handling the non-linearity in the data, where linear models performed poorly, while tree-based models handled it well without requiring feature scaling. Additionally, tuning hyperparameters and managing model complexity, especially with SVR and tree-based methods, was crucial to avoid overfitting and improve performance.\n",
        "\n",
        "2. **Features on Different Scales**\n",
        "\n",
        "  **Problem:** Input features like cement, water, and age had vastly different ranges, which could mislead certain models (especially SVR and linear models).\n",
        "  \n",
        "  **Impact:** Poor convergence and lower accuracy.\n",
        "\n",
        " **Solution:** We applied StandardScaler to bring all features onto the same scale. This significantly improved the performance of SVR and linear regression\n"
      ],
      "metadata": {
        "id": "u2Dr4GEM0msM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-QUQIXcDK0NG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}